---
title: "Intro to Regression Analysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Welcome

In this workshop, We will explore the basic regression analysis in R. The datasets consist of the `spotify_lyrics` data modified from [the kaggle dataset](https://www.kaggle.com/zaheenhamidani/ultimate-spotify-tracks-db) and the `spotify` data comprised of the musical attributes of weekly [Billboard Hot 100](https://www.billboard.com/charts/billboard-200/) songs from February 2019 to February 2021. Both of the datasets were mined using the [spotifyr](https://www.rcharlie.com/spotifyr/) interface to the [Spotify Web API](https://developer.spotify.com/documentation/web-api/)*. We will have to load the following packages in this workshop: `tidyverse`,`GGally`,`leaps`. 

Learning objectives: 

- Exploratory data analysis

- Fit regression models with the `lm` function 

- Fit regression models with the `glm` function  

In the future: 

- Automatic model selection (package `leaps`)

- Prediction

- Interaction terms 

Along the way, you will master the core functions for running linear regression in R and be able to interpret the regression result generated in R: 

- `pairs()` and `ggpairs()`, which let you explore the relationships among variables 

- `glimpse` and `summary()`, which let you understand the structure of the dataset and descriptive statistics of the variables 

- `%>%`, which organizes your code into reader-friendly "pipes" 

- `step`, which helps with automatic model selection 

First, let's load the `tidyverse` suite of packages and the `here` package. The `here` package simplifies the way of reproducing file paths. 

```{r load-package, warning = FALSE, message= FALSE, error = FALSE}
library(tidyverse)
library(here)
library(GGally)
library(leaps)
```

### Spotify data
We'll first analyze a Spotify dataset and the variables are:

- energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy

- loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks

- tempo: The overall estimated tempo of a track in beats per minute (BPM)

- danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity

- lyrics: lyrics in the song                                       

More details about the variables can be found at [here](https://github.com/YuxiaoLuo/r_analysis_dri_2022/blob/main/data/meta_spotify.md). In our case, the dependent/response variable will be **energy**. 

Let's read the dataset first. You can load it either online or locally. 

```{r load-spotify, message=FALSE, paged.print=TRUE}
# if you haven't downloaded the dataset, load it online
spo <- read_csv("https://raw.githubusercontent.com/YuxiaoLuo/r_analysis_dri_2022/main/data/spotify_lyrics.csv")

# if you have the dataset locally, load it in the data folder
spo <- read_csv(here("data", "spotify_lyrics.csv"))

glimpse(spo)
```

### Exploratory data analysis
The function `pairs` creates a scatterplot matrix for **numeric** variables:

```{r scatterplot}
spo_plot <- spo %>% select(-genre, -lyrics)
pairs(spo_plot)
```

The dataset **spo_plot** excludes the variables **genre** and **lyrics**. We can get quick and dirty summaries of the variables with summary. An advantage is that it's able to handle categorical variables, such as **genre**: 

```{r}
summary(spo)
```

The function `ggpairs` in `library(GGally)` produces the equivalent plot, but with `ggplot2`: 

```{r, message = FALSE}
ggpairs(spo_plot)
```

### Fitting regression models with the lm function
Fitting regression models with R is easy as R was developed originally as a statistical/mathematical computing language. For example, we can fit a model where the response variable is **energy** and the predictors are **tempo** and **loudness**. 


```{r fitting-regression}
lm_spo <- lm(energy ~ tempo + loudness, spo)

lm_spo
```

If we call the object `lm_spo`, it will return us coefficients. If we want details about p-values, R-squared, and more, we can get them with `summary()`.

```{r regression-result}
summary(lm_spo)
```

We can get diagnostic plots by plotting the model. That will give us 4 diagnostic plots. We can arrange them in a figure with x rows and y columns with `par(mfrow=c(x,y))`.

```{r plotting1}
par(mfrow=c(2,2))
plot(lm_spo)
```

Plot 1 in the top left (Residuals vs Fitted) shows:
  1. Whether linearity holds. This is indicated by the mean residual value for every fitted value region being close to 0. In R this is indicated by the red line being close to the dashed line.
  2. Whether homoskedasticity holds. The spread of residuals should be approximately the same across the x-axis.
  3. Whether there are outliers. This is indicated by some ‘extreme’ residuals that are far from the rest. [Reference](https://boostedml.com/2019/03/linear-regression-plots-fitted-vs-residuals.html)
  
Plot 2 in the top right (Q-Q Plot), is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a Normal or exponential. Ex., If both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight. More detail is [here](https://data.library.virginia.edu/understanding-q-q-plots/).  
  
Plot 4 in the bottom right (Residuals vs Leverage) allows us to identify influential observations in a regression model. You can see the number of observation that is close to or exceed the Cook's distance in the graph. More detail is [here](https://www.statology.org/residuals-vs-leverage-plot/).

If we want them in 1 row and 4 columns: 

```{r plotting2}
par(mfrow=c(1,4))
plot(lm_spo)
```

The `par(mfrow=c(x,y))` function can arrange figures with multiple rows and columns of plots in `library(graphics)`. But it doesn't work with `ggplot2`, in which `grid.arrange` is used to do the same thing. 

We can extract diagnostics from the regression model. For example, if we are interested in Cook's distances, which is useful for identifying influential data points and outliers in the X values, we can extract Cook's distances and plot them against observation number. 

A general rule of thumb is that any point with a Cook's distance over 4/n (where n is the number of observations) is considered to be an outlier. Please note that just because a data point is influential doesn't mean it should necessarily be deleted. You can find more the interpretation of it [here](https://www.statology.org/how-to-identify-influential-data-points-using-cooks-distance/).

```{r cookdistance}
cookd <- cooks.distance(lm_spo)
plot(cookd)
abline(h = 4/nrow(spo), lty = 2, col = "steelblue") # add cutoff line
```

Other useful functions are `hatvalues` (for leverages), `residuals` (for residuals), and `rstandard` (for standardized residuals). 

Leverage is used to identify outliers with respect to the independent variables and high-leverage points have the potential to cause large changes in the parameter estimates when they are deleted. More details about leverage click [here](https://handwiki.org/wiki/Leverage_(statistics)).

Another important assumption check is the multicollinearity of the independent variables. We can use `vif()` in `library(car)` to test it. 

```{r vif, warning=FALSE}
car::vif(lm_spo)
```

The rule of thumb is that VIFs exceeding 4 warrant further investigation, while VIFs exceeding 10 are signs of serious multicollinearity. More statistical details can be found [here](https://online.stat.psu.edu/stat462/node/180/).

### Practice of fitting a regression model

#### Problem 1
Here, we have a NYC restaurant dataset for you to practice what we have just learned. You'll analyze a dataset in Sheather (2009) that has information about 150 Italian restaurants in Manhattan that were open in 2001 (some of them are closed now). The variables are:

- Case: case-indexing variable 

- Restaurant: name of the restaurant 

- Price: average price of meal and a drink 

- Food: average Zagat rating of the quality of the food (from 0 to 25)

- Decor: same as above, but with quality of the decor

- Service: same as above, but with quality of service

- East: it is equal to East if the restaurant is on the East Side (i.e. east of Fifth Ave) and West otherwise.

In the practice problem, the response variable will be **Price** and the predictors are **Food**, **Decor**, **Service**, **East**. You will have to:

1. Load the dataset in R.

  1. Using the link: `read_csv('https://raw.githubusercontent.com/YuxiaoLuo/r_analysis_dri_2022/main/data/nyc.csv')`
  
  2. Or load it locally: `read_csv(here('data', 'nyc.csv'))` 

2. Explore the relationship between variables

3. Run a linear regression model and generate the regression result

#### Problem 2

Here we have another `cars` dataset located in the `library(caret)`, which is the Kelly Blue Book resale data for 2005 model year GM cars. Since this dataset is built inside the package, we can load it using `data(cars)`. There are many other datasets in `caret` and you can check them out [here](https://topepo.github.io/caret/data-sets.html).

```{r problem2-import, message=FALSE, warning=FALSE, paged.print=TRUE}
library(caret)
data(cars)
glimpse(cars)
```

What you need to do: 

1. Load the dataset following the code above. 

2. Try to find out the variable description using the Help window in R. (Hint: `?caret::cars`)

3. Fit a regression model to explain/predict the **Price** of the GM cars. 

4. Think about other regression models that makes sense to you and choose the one with the best performance ($R^2$). 

5. Does your model perform better than the one I proposed (solution at the end of this page)? 

### Fitting Logistic Regression with the glm function

Logistic regression is used when the response variable is a binary (ex., 0 and 1). We will use a `titanic` dataset to demonstrate how to run a logistic regression in R using `glm` function. The variables are: 

1. survived: Survival (0 = No; 1 = Yes)

2. sex: Sex

3. age: Age

4. Pclass: Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)

We will use **survived** as response variable, and the other 3 as independent variables. 

```{r glm-import, message=FALSE}
# if you haven't downloaded the dataset, load it online
tita <- read_csv("https://raw.githubusercontent.com/YuxiaoLuo/r_analysis_dri_2022/main/data/titanic_train.csv")

# if you have the dataset locally, load it in the data folder
tita <- read_csv(here("data","titanic_train.csv"))
```

Explore the variables in `titanic` dataset. 

```{r glm-glimpse, paged.print=TRUE}
glimpse(tita)
```

Visualize the relationships among the variables. 

```{r glm-explore, message = FALSE}
GGally::ggpairs(tita, axisLabels = "show")
```

Fit a logistic regression using `glm` function. We need to specify the residual distribution and link function to be used in the model in the parameter `family`. Once we specify `binomial` distribution, `logit` is the default link function. For example, if you want to do probit regression, you will specify `probit` in the link function as `family = binomial(link = "probit")`. 

```{r glm-function}
mod <- glm(Survived ~ Pclass + Age + Sex, family = "binomial", data = tita)
summary(mod)
```

From the regression result, we see that **Pclass**, **Age**, and **Sex** are all statistically significant. The AIC score is 457.25. 

`vip` is an R package for constructing variable importance (VI) scores/plots for many type of supervised learning algorithms using model-specific and novel model-agnostic approaches. 

```{r importance1, warning=FALSE, paged.print=TRUE}
library(vip)
vi(mod)
```

`vip` function can create a variable importance barplot, which is based on `ggplot2`. 
```{r importance2}
vip(mod)
```

If there are many variables (here we only have 3), you can change barplot to points and make the visualization more readable by changing it from vertical to horizontal and add color to the points based on the sign of the importance score. 

```{r importance3}
vip(mod, num_features = length(coef(mod)), geom = "point", horizontal = FALSE, mapping = aes_string(color = "Sign"))
```

This `vip` package can be used to many other machine learning algorithms too, ex., neural network, random forest, support vector machine, etc. This [article](https://koalaverse.github.io/vip/articles/vip.html) introduces its usage more thoroughly. This [paper](https://cran.r-project.org/web/packages/vip/vignettes/vip-introduction.pdf) explains the computational mechanism in detail. 

### Solutions to the Practice Problems

#### Problem 1
Load the dataset into R and take a look at the variables. 

```{r problem-read, message=FALSE}
nyc <- read_csv('https://raw.githubusercontent.com/YuxiaoLuo/r_analysis_dri_2022/main/data/nyc.csv')
glimpse(nyc)
```

Further explore the data with `summary` and `ggpairs`.

```{r problem-summary}
summary(nyc)
```

Create a dataset nycplot excludes the variables Case, Restaurant, and East. 

```{r problem-matrix, message=FALSE}
nycplot <- nyc %>% select(-Case, -Restaurant, -East)
ggpairs(nycplot)
```

Do you see any interesting patterns in the matrix? Now, let's run the regression based on the proposed model, where **Price** is the dependent variable and the predictors are **Food**, **Decor**, **Service**, and **East**. 

```{r problem-model}
lm_nyc <- lm(Price ~ Food + Decor + Service + East, data = nyc)
summary(lm_nyc)
```

Run the diagnostic graphs and VIF checks.

```{r problem-check}
par(mfrow=c(2,2))
plot(lm_nyc)
```

```{r problem-checkVIF}
car::vif(lm_nyc)
```

Read the regression result, what conclusions can you draw from it? 

#### Problem 2

In my model, the response variable is **Price** and the independent variables are **Mileage**, **Cylinder**, **Doors**, **Cruise**. 

```{r car-model}
lm_cars <- lm(Price ~ Mileage + Cylinder + Doors + Cruise, data = cars)
summary(lm_cars)
```

Compare your model with this one, which one performs better? 








